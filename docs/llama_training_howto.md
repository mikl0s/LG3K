# Training Llama with LG3K: A Step-by-Step Guide

This guide will walk you through the process of training `llama3.2:3b-instruct-fp16` using logs generated by LG3K. The guide is optimized for systems with 8-12GB NVIDIA GPU VRAM.

## Prerequisites

### Hardware Requirements
- NVIDIA GPU with 8-12GB VRAM
- 16GB+ System RAM
- 50GB+ Free Disk Space

### Software Requirements
- Windows 11, macOS Sequoia, or Ubuntu 24.04
- Python 3.12+
- NVIDIA CUDA Toolkit 12.3+
- NVIDIA GPU Drivers (Latest)

## Step 1: Environment Setup

### Windows 11
```powershell
# Install CUDA Toolkit from NVIDIA Website
# Download: https://developer.nvidia.com/cuda-downloads
# Select: Windows -> x86_64 -> 11 -> exe(network)

# Install Python 3.12
winget install Python.Python.3.12

# Create workspace directory
mkdir llama-training
cd llama-training

# Create virtual environment
python -m venv venv
.\venv\Scripts\activate

# Install LG3K
pip install lg3k
```

### macOS Sequoia
```bash
# Install Homebrew if not installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Python and CUDA
brew install python@3.12
brew install cuda

# Create workspace directory
mkdir llama-training
cd llama-training

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install LG3K
pip install lg3k
```

### Ubuntu 24.04
```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install CUDA and drivers
sudo apt install nvidia-driver-545 nvidia-cuda-toolkit

# Install Python
sudo apt install python3.12 python3.12-venv

# Create workspace directory
mkdir llama-training
cd llama-training

# Create virtual environment
python3.12 -m venv venv
source venv/bin/activate

# Install LG3K
pip install lg3k
```

## Step 2: Install Ollama

### Windows 11
1. Download Ollama installer from [ollama.com](https://ollama.com)
2. Run the installer
3. Open PowerShell and verify installation:
   ```powershell
   ollama --version
   ```

### macOS Sequoia
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Ubuntu 24.04
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Step 3: Generate Training Data

```bash
# Create logs directory
mkdir logs

# Generate web server logs
lg3k --count 10000 --threads 4 --llm-format --json-output --output-dir logs --services web_server

# Generate database logs
lg3k --count 10000 --threads 4 --llm-format --json-output --output-dir logs --services database

# Generate API logs
lg3k --count 10000 --threads 4 --llm-format --json-output --output-dir logs --services api
```

This will generate:
- 10,000 web server logs
- 10,000 database logs
- 10,000 API logs
All in instruction-tuning format optimized for Llama.

## Step 4: Download and Configure Llama

```bash
# Pull the model
ollama pull llama3.2:3b-instruct-fp16

# Verify model
ollama list
```

## Step 5: Prepare Training Script

Create a file named `train.py`:

```python
import os
import json
import ollama
from pathlib import Path

def load_training_data(logs_dir):
    """Load and format training data from logs."""
    training_data = []
    for file in Path(logs_dir).glob('*.jsonl'):
        with open(file) as f:
            for line in f:
                entry = json.loads(line)
                training_data.append({
                    'prompt': f"{entry['instruction']}\n\nInput: {entry['input']}",
                    'response': entry['output']
                })
    return training_data

def train_model(data, batch_size=4):
    """Train the model with the provided data."""
    model = 'llama3.2:3b-instruct-fp16'

    for i, entry in enumerate(data):
        try:
            response = ollama.chat(
                model=model,
                messages=[{'role': 'user', 'content': entry['prompt']}],
                stream=False
            )
            print(f"Training progress: {i+1}/{len(data)}")
        except Exception as e:
            print(f"Error on entry {i}: {str(e)}")
            continue

if __name__ == '__main__':
    data = load_training_data('logs')
    train_model(data)
```

## Step 6: Start Training

```bash
# Activate virtual environment if not already active
# Windows:
.\venv\Scripts\activate
# macOS/Ubuntu:
source venv/bin/activate

# Start training
python train.py
```

## Step 7: Monitor Training

The training script will show progress for each batch. Monitor:
- GPU memory usage (should stay under 10GB)
- Training progress
- Any error messages

### GPU Monitoring Commands

#### Windows
```powershell
nvidia-smi -l 1
```

#### macOS/Ubuntu
```bash
watch -n 1 nvidia-smi
```

## Step 8: Test the Model

After training, test the model with some sample logs:

```python
import ollama

# Test with a sample log
response = ollama.chat(
    model='llama3.2:3b-instruct-fp16',
    messages=[{
        'role': 'user',
        'content': 'Analyze this log entry: "GET /api/v1/users 404 Not Found"'
    }]
)
print(response['message']['content'])
```

## Troubleshooting

### Common Issues

1. **GPU Out of Memory**
   - Reduce batch size in train.py
   - Close other GPU-intensive applications
   - Ensure no other models are loaded

2. **Training Too Slow**
   - Check GPU utilization with nvidia-smi
   - Increase batch size if memory allows
   - Ensure CUDA is properly installed

3. **Model Loading Fails**
   - Verify Ollama installation
   - Check GPU drivers are up to date
   - Ensure enough disk space

### Memory Usage Guidelines

- Model Base Size: ~6GB VRAM
- Training Overhead: ~2GB VRAM
- Safe Batch Size: 4-8 entries
- Total VRAM Required: 8-10GB

## Next Steps

1. Experiment with different log types
2. Adjust batch sizes based on your GPU
3. Try different instruction formats
4. Test model performance on new logs
5. Fine-tune hyperparameters

## References

- [Ollama Documentation](https://ollama.com/docs)
- [Developer Guide](developer_guide.md)
- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/)
