# LLM Training Guide for LG3K Logs

This guide explains how to train Large Language Models (LLMs) using logs generated by LG3K.

## Overview

LG3K can generate logs in a format optimized for LLM training using the `--llm-format` option. This automatically structures the logs as instruction-input-output triplets suitable for training.

## Prerequisites

- Python (latest stable version)
- CUDA Toolkit (latest stable version) for GPU training
- Git (latest version)
- LG3K (latest version)
- Ollama (latest version)
- 8-12GB VRAM (NVIDIA GPU) recommended, CPU fallback available

## Installation

1. Install Ollama:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. Pull the base model:
   ```bash
   ollama pull llama3.2:1b-instruct-fp16
   ```

## Generating Training Data

LG3K's `--llm-format` option automatically generates logs optimized for LLM training:

```bash
lg3k --llm-format
```

This will:
- Automatically detect available GPU/CPU memory
- Calculate optimal batch sizes and thread counts
- Generate balanced datasets across log types
- Include API, database, and web server logs
- Structure data as instruction-input-output triplets

### Automatic Optimization

When using `--llm-format`, LG3K:
1. Detects available hardware:
   - GPU memory (if available)
   - System memory
   - CPU cores

2. Calculates optimal parameters:
   - Batch size (based on memory)
   - Thread count (based on cores)
   - Sample count (for multiple epochs)
   - Training hyperparameters

3. Configures training settings:
   - Warmup steps (10% of batch size)
   - Save checkpoints (every 5% of batch size)
   - Evaluation frequency (every 2.5% of batch size)
   - Validation split (10% of data)

## Automated Training Loop

Here's a script to automate the training process:

```python
import subprocess
import time
import json
from pathlib import Path

def generate_logs():
    # Run LG3K with automatic optimization
    result = subprocess.run(
        ["lg3k", "--llm-format", "--json-output"],
        capture_output=True,
        text=True
    )
    config = json.loads(result.stdout)["config"]["training"]
    return config

def train_model(config):
    # Use optimized configuration
    subprocess.run([
        "ollama", "train", "llama3.2:1b-instruct-fp16",
        "--training-data", "logs/*.jsonl",
        "--batch-size", str(config["batch_size"]),
        "--epochs", str(config["epochs"]),
        "--save-steps", str(config["save_steps"]),
        "--eval-steps", str(config["eval_steps"])
    ])

def evaluate_model():
    # Add your evaluation logic here
    pass

def main():
    while True:
        print("Generating fresh logs with optimal configuration...")
        config = generate_logs()

        print(f"Training model with batch size {config['batch_size']}...")
        train_model(config)

        print("Evaluating model...")
        evaluate_model()

        print("Waiting for next iteration...")
        time.sleep(3600)  # Wait 1 hour between iterations

if __name__ == "__main__":
    main()
```

This script:
1. Generates logs with optimal configuration
2. Uses the detected parameters for training
3. Evaluates performance
4. Repeats the process

## Training Configuration

When using `--llm-format`, LG3K automatically configures:

1. **Hardware Optimization**:
   - GPU: Uses 80% of available VRAM
   - CPU: Uses 40% of system memory
   - Threads: One per GB of memory, up to CPU core count

2. **Batch Sizing**:
   - GPU: Up to 128 samples per batch
   - CPU: Up to 64 samples per batch
   - Fallback: 32 samples if detection fails

3. **Training Parameters**:
   - Sequence length: 512 tokens
   - Epochs: 10
   - Validation split: 10%
   - Dynamic warmup steps
   - Adaptive checkpointing

The generated logs follow this format:
```json
{
    "instruction": "Analyze this log entry and identify any anomalies",
    "input": "<log entry>",
    "output": "<analysis>"
}
```

## Model Training

Start training with:
```bash
ollama train llama3.2:1b-instruct-fp16 --training-data logs/*.jsonl
```

For GPU training (recommended):
```bash
CUDA_VISIBLE_DEVICES=0 ollama train llama3.2:1b-instruct-fp16 --training-data logs/*.jsonl
```

For CPU training (fallback):
```bash
CUDA_VISIBLE_DEVICES=-1 ollama train llama3.2:1b-instruct-fp16 --training-data logs/*.jsonl
```

## Evaluation

Monitor these metrics during training:
- Loss convergence
- Prediction accuracy on validation logs
- Response quality on unseen log types
- Training speed (samples/second)
- Memory usage and batch completion rate

## Best Practices

1. **Data Generation**:
   - Let LG3K handle optimization
   - Monitor memory usage during generation
   - Generate fresh logs periodically
   - Use the `--json-output` flag to get configuration

2. **Training**:
   - Start with default optimization
   - Adjust batch size if needed
   - Monitor GPU/CPU utilization
   - Use automated training loop

3. **Validation**:
   - Test on unseen log types
   - Verify analysis quality
   - Check for overfitting
   - Monitor validation loss

## Troubleshooting

Common issues and solutions:
- GPU out of memory: LG3K will automatically adjust batch size
- Poor convergence: Generate fresh logs with different patterns
- Slow training: Check hardware utilization
- Memory errors: Reduce thread count or batch size

## Next Steps

1. Start with automatic optimization
2. Monitor resource usage
3. Adjust parameters if needed
4. Use continuous training loop
